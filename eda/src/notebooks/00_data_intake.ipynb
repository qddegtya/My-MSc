{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e386af",
   "metadata": {},
   "source": [
    "# 00 数据接入与类型规范"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76da4c2",
   "metadata": {},
   "source": [
    "目标：将原始 CSV 流式加载、统一字段类型、合并作者元数据，并写入 Parquet 缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0419ff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Python 路径已配置: /workspace\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 将项目根目录添加到 Python 路径\n",
    "project_root = Path('/workspace')\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    \n",
    "print(f\"✅ Python 路径已配置: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578ec70",
   "metadata": {},
   "source": [
    "## 步骤 1: 加载原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2e15c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 数据 schema:\n",
      "Schema([('pseudo_id', Int64), ('text', String), ('retweetCount', Int64), ('replyCount', Int64), ('likeCount', Int64), ('quoteCount', Int64), ('viewCount', Int64), ('bookmarkCount', Int64), ('createdAt', String), ('lang', String), ('isReply', String), ('pseudo_conversationId', Int64), ('pseudo_inReplyToUsername', String), ('pseudo_author_userName', Int64), ('quoted_pseudo_id', String), ('author_isBlueVerified', String)])\n"
     ]
    }
   ],
   "source": [
    "from src import io, analysis, profiling\n",
    "import polars as pl\n",
    "\n",
    "# 加载原始推文数据 (LazyFrame)\n",
    "raw_lf = io.scan_raw_tweets()\n",
    "print(f\"📊 数据 schema:\")\n",
    "print(raw_lf.collect_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f67d4",
   "metadata": {},
   "source": [
    "## 步骤 2: 数据清洗与类型规范化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee8f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据加载完成: 508,954 行, 16 列\n",
      "✅ 布尔列规范化完成: ['isReply', 'author_isBlueVerified']\n"
     ]
    }
   ],
   "source": [
    "# 收集数据并规范化布尔列\n",
    "df = raw_lf.collect()\n",
    "print(f\"✅ 数据加载完成: {df.height:,} 行, {df.width} 列\")\n",
    "\n",
    "# 规范化布尔列\n",
    "bool_columns = ['isReply', 'author_isBlueVerified']\n",
    "df_cleaned = analysis.normalize_boolean_columns(df, bool_columns)\n",
    "print(f\"✅ 布尔列规范化完成: {bool_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e007ef6",
   "metadata": {},
   "source": [
    "## 步骤 3: 数据质量检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58352573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 缺失值统计 (top 5):\n",
      "shape: (5, 4)\n",
      "┌──────────────┬────────────┬────────────┬────────┐\n",
      "│ column       ┆ null_count ┆ null_ratio ┆ is_key │\n",
      "│ ---          ┆ ---        ┆ ---        ┆ ---    │\n",
      "│ str          ┆ i64        ┆ f64        ┆ bool   │\n",
      "╞══════════════╪════════════╪════════════╪════════╡\n",
      "│ pseudo_id    ┆ 0          ┆ 0.0        ┆ true   │\n",
      "│ text         ┆ 0          ┆ 0.0        ┆ true   │\n",
      "│ retweetCount ┆ 0          ┆ 0.0        ┆ false  │\n",
      "│ replyCount   ┆ 0          ┆ 0.0        ┆ false  │\n",
      "│ likeCount    ┆ 0          ┆ 0.0        ┆ false  │\n",
      "└──────────────┴────────────┴────────────┴────────┘\n",
      "\n",
      "🔍 重复推文数: 50\n"
     ]
    }
   ],
   "source": [
    "# 缺失值检查\n",
    "key_cols = ['pseudo_id', 'pseudo_author_userName', 'createdAt', 'text']\n",
    "missing_stats = profiling.missingness_summary(df_cleaned, key_cols)\n",
    "print(\"📊 缺失值统计 (top 5):\")\n",
    "print(missing_stats.head(5))\n",
    "\n",
    "# 重复检查\n",
    "dupes = profiling.duplicate_check(df_cleaned, ['pseudo_id'])\n",
    "print(f\"\\n🔍 重复推文数: {dupes.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c483149",
   "metadata": {},
   "source": [
    "## 步骤 4: 合并作者元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652f8496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 作者元数据: 4242 位作者\n",
      "✅ 数据合并完成: 508,954 行, 23 列\n"
     ]
    }
   ],
   "source": [
    "# 加载作者信息\n",
    "authors_df = io.read_well_known_authors()\n",
    "print(f\"📋 作者元数据: {authors_df.height} 位作者\")\n",
    "\n",
    "# 类型转换：将 pseudo_author_userName 转为字符串以匹配 author_userName\n",
    "df_with_str_author = df_cleaned.with_columns(\n",
    "    pl.col('pseudo_author_userName').cast(pl.Utf8).alias('pseudo_author_userName')\n",
    ")\n",
    "\n",
    "# 合并推文和作者数据（基于用户名）\n",
    "df_with_authors = df_with_str_author.join(\n",
    "    authors_df, \n",
    "    left_on='pseudo_author_userName', \n",
    "    right_on='author_userName',\n",
    "    how='left'\n",
    ")\n",
    "print(f\"✅ 数据合并完成: {df_with_authors.height:,} 行, {df_with_authors.width} 列\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471acde2",
   "metadata": {},
   "source": [
    "## 步骤 5: 写入 Parquet 缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4c0293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parquet 文件已生成: parquet/tweets_enriched.parquet\n",
      "📁 文件大小: 58.00 MB\n",
      "\n",
      "📂 生成的 Parquet 文件:\n",
      "  - /workspace/src/notebooks/parquet/content_analysis.parquet\n",
      "  - /workspace/src/notebooks/parquet/network_centrality.parquet\n",
      "  - /workspace/src/notebooks/parquet/network_edges.parquet\n",
      "  - /workspace/src/notebooks/parquet/topic_distribution.parquet\n",
      "  - /workspace/src/notebooks/parquet/tweets_anomalies.parquet\n",
      "  - /workspace/src/notebooks/parquet/tweets_daily.parquet\n",
      "  - /workspace/src/notebooks/parquet/tweets_enriched.parquet\n",
      "  - /workspace/src/notebooks/parquet/tweets_rolling.parquet\n",
      "  - /workspace/src/notebooks/parquet/verified_comparison.parquet\n"
     ]
    }
   ],
   "source": [
    "# 写入 parquet 文件供后续分析使用\n",
    "output_path = Path(\"parquet/tweets_enriched.parquet\")\n",
    "io.materialize_parquet(df_with_authors.lazy(), output_path)\n",
    "\n",
    "print(f\"✅ Parquet 文件已生成: {output_path}\")\n",
    "print(f\"📁 文件大小: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# 列出所有生成的 parquet 文件\n",
    "print(\"\\n📂 生成的 Parquet 文件:\")\n",
    "for f in io.list_parquet_files():\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66e626",
   "metadata": {},
   "source": [
    "## ✅ 数据接入完成！\n",
    "\n",
    "接下来可以运行 `01_temporal_dynamics.ipynb` 进行时间序列分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
