{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 03 å†…å®¹åˆ†æï¼šæƒ…æ„Ÿã€å™äº‹ä¸æ”¿æ²»ç«‹åœº\n",
    "\n",
    "**ç ”ç©¶æ ¸å¿ƒ**: Charlie Kirkæ”¿æ²»æš—æ€äº‹ä»¶å72å°æ—¶çš„ç¤¾äº¤åª’ä½“èˆ†è®ºå†…å®¹åˆ†æ\n",
    "\n",
    "**åˆ†æç»´åº¦**:\n",
    "1. **6ç»´æƒ…æ„Ÿåˆ†æ**: sadness, anger, fear, surprise, disgust, joy\n",
    "2. **6å¤§å™äº‹æ¡†æ¶**: æ”¿æ²»æš´åŠ›å—å®³è€…ã€è¨€è®ºåæœã€æ”¿æ²»æåŒ–ã€è¨€è®ºè‡ªç”±ã€é˜´è°‹è®ºã€çºªå¿µé—äº§\n",
    "3. **æ”¿æ²»ç«‹åœºåˆ†ç±»**: conservative, liberal, neutral\n",
    "4. **æ—¶é—´æ¼”å˜**: æƒ…æ„Ÿä¸å™äº‹éš72å°æ—¶çš„å˜åŒ–\n",
    "5. **ä»£è¡¨æ€§å†…å®¹**: æ¯ç±»å™äº‹çš„å…¸å‹æ¨æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Python è·¯å¾„å·²é…ç½®: /workspace\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„\n",
    "project_root = Path('/workspace')\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    \n",
    "print(f\"âœ… Python è·¯å¾„å·²é…ç½®: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 1: åŠ è½½æ•°æ®å¹¶é‡‡æ ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: 508,954 è¡Œ\n",
      "ğŸ“ æœ‰æ•ˆè‹±æ–‡æ¨æ–‡: 415,714 æ¡\n",
      "  24-48h: 337,119 æ¡ â†’ é‡‡æ · 2,000 æ¡\n",
      "  48-72h: 78,595 æ¡ â†’ é‡‡æ · 2,000 æ¡\n",
      "\n",
      "ğŸ“‹ é‡‡æ ·å®Œæˆ: 4,000 æ¡æ¨æ–‡\n",
      "\n",
      "æ—¶é—´çª—å£åˆ†å¸ƒ:\n",
      "shape: (2, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ time_window â”† count â”‚\n",
      "â”‚ ---         â”† ---   â”‚\n",
      "â”‚ str         â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ 24-48h      â”† 2000  â”‚\n",
      "â”‚ 48-72h      â”† 2000  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# åŠ è½½enrichedæ•°æ®ï¼ˆåŒ…å«event_time_delta_hourså’Œtime_windowå­—æ®µï¼‰\n",
    "df = pl.read_parquet(\"../parquet/tweets_enriched.parquet\")\n",
    "print(f\"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {df.height:,} è¡Œ\")\n",
    "\n",
    "# è¿‡æ»¤æœ‰æ•ˆè‹±æ–‡æ–‡æœ¬\n",
    "df_text = df.filter(\n",
    "    (pl.col('text').is_not_null()) & \n",
    "    (pl.col('lang') == 'en') &\n",
    "    (pl.col('text').str.len_chars() > 20)  # è‡³å°‘20å­—ç¬¦\n",
    ")\n",
    "print(f\"ğŸ“ æœ‰æ•ˆè‹±æ–‡æ¨æ–‡: {df_text.height:,} æ¡\")\n",
    "\n",
    "# é‡‡æ ·ç­–ç•¥ï¼šæ¯ä¸ªæ—¶é—´çª—å£é‡‡æ ·æœ€å¤š20000æ¡ï¼ˆç¡®ä¿æ—¶é—´æ¼”å˜åˆ†æçš„ä»£è¡¨æ€§ï¼‰\n",
    "sample_per_window = 20000\n",
    "\n",
    "# æ–¹æ³•ï¼šå¯¹æ¯ä¸ªæ—¶é—´çª—å£åˆ†åˆ«é‡‡æ ·ååˆå¹¶\n",
    "sampled_dfs = []\n",
    "for window in df_text['time_window'].unique().sort():\n",
    "    window_df = df_text.filter(pl.col('time_window') == window)\n",
    "    # å¦‚æœè¯¥çª—å£æ•°æ®å°‘äº20000ï¼Œå…¨éƒ¨ä½¿ç”¨ï¼›å¦åˆ™é‡‡æ ·20000\n",
    "    n_sample = min(sample_per_window, window_df.height)\n",
    "    sampled = window_df.sample(n=n_sample, seed=42)\n",
    "    sampled_dfs.append(sampled)\n",
    "    print(f\"  {window}: {window_df.height:,} æ¡ â†’ é‡‡æ · {n_sample:,} æ¡\")\n",
    "\n",
    "df_sample = pl.concat(sampled_dfs).sort('createdAt')\n",
    "\n",
    "print(f\"\\nğŸ“‹ é‡‡æ ·å®Œæˆ: {df_sample.height:,} æ¡æ¨æ–‡\")\n",
    "print(f\"\\næ—¶é—´çª—å£åˆ†å¸ƒ:\")\n",
    "print(df_sample.group_by('time_window').agg(pl.len().alias('count')).sort('time_window'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotion_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 2: å…­ç»´æƒ…æ„Ÿåˆ†æ\n",
    "\n",
    "ä½¿ç”¨ HuggingFace `j-hartmann/emotion-english-distilroberta-base` æ¨¡å‹  \n",
    "åˆ†ç±»ï¼šsadness, joy, love, anger, fear, surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "emotion_analysis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹åŠ è½½å®Œæˆ (device: CPU)\n",
      "\n",
      "ğŸ”„ å¼€å§‹æƒ…æ„Ÿåˆ†æ (4,000 æ¡æ¨æ–‡)...\n",
      "âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ\n",
      "\n",
      "ğŸ“Š æƒ…æ„Ÿåˆ†å¸ƒ:\n",
      "shape: (7, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ primary_emotion â”† count â”‚\n",
      "â”‚ ---             â”† ---   â”‚\n",
      "â”‚ str             â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ anger           â”† 1026  â”‚\n",
      "â”‚ neutral         â”† 867   â”‚\n",
      "â”‚ fear            â”† 821   â”‚\n",
      "â”‚ sadness         â”† 543   â”‚\n",
      "â”‚ surprise        â”† 333   â”‚\n",
      "â”‚ joy             â”† 247   â”‚\n",
      "â”‚ disgust         â”† 163   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(\"ğŸ¤– åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    device=device,\n",
    "    top_k=None  # è¿”å›æ‰€æœ‰æƒ…æ„Ÿçš„æ¦‚ç‡\n",
    ")\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (device: {'GPU' if device == 0 else 'CPU'})\")\n",
    "\n",
    "# å¤„ç†æ–‡æœ¬ï¼ˆæ‰¹é‡æ¨ç†ï¼‰\n",
    "texts = df_sample['text'].to_list()\n",
    "print(f\"\\nğŸ”„ å¼€å§‹æƒ…æ„Ÿåˆ†æ ({len(texts):,} æ¡æ¨æ–‡)...\")\n",
    "\n",
    "# æ‰¹é‡å¤„ç†ï¼Œæ¯æ‰¹128æ¡\n",
    "batch_size = 128\n",
    "all_emotions = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    # æˆªæ–­é•¿æ–‡æœ¬\n",
    "    batch_truncated = [t[:512] for t in batch]\n",
    "    results = emotion_classifier(batch_truncated)\n",
    "    all_emotions.extend(results)\n",
    "    \n",
    "    if (i + batch_size) % 1000 == 0:\n",
    "        print(f\"  å¤„ç†è¿›åº¦: {i + batch_size:,} / {len(texts):,}\")\n",
    "\n",
    "print(f\"âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ\")\n",
    "\n",
    "# æå–ä¸»è¦æƒ…æ„Ÿå’Œç½®ä¿¡åº¦\n",
    "primary_emotions = [max(e, key=lambda x: x['score'])['label'] for e in all_emotions]\n",
    "primary_scores = [max(e, key=lambda x: x['score'])['score'] for e in all_emotions]\n",
    "\n",
    "# æå–6å¤§æƒ…æ„Ÿçš„åˆ†æ•°ï¼ˆæ„å»ºæƒ…æ„Ÿå‘é‡ï¼‰\n",
    "emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "emotion_vectors = {}\n",
    "for label in emotion_labels:\n",
    "    scores = []\n",
    "    for result in all_emotions:\n",
    "        score_dict = {item['label']: item['score'] for item in result}\n",
    "        scores.append(score_dict.get(label, 0.0))\n",
    "    emotion_vectors[f'emotion_{label}'] = scores\n",
    "\n",
    "# æ·»åŠ åˆ°dataframe\n",
    "df_sample = df_sample.with_columns([\n",
    "    pl.Series('primary_emotion', primary_emotions),\n",
    "    pl.Series('emotion_confidence', primary_scores),\n",
    "    *[pl.Series(k, v) for k, v in emotion_vectors.items()]\n",
    "])\n",
    "\n",
    "print(f\"\\nğŸ“Š æƒ…æ„Ÿåˆ†å¸ƒ:\")\n",
    "print(df_sample.group_by('primary_emotion').agg(pl.len().alias('count')).sort('count', descending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 3: å…­å¤§å™äº‹æ¡†æ¶æ£€æµ‹\n",
    "\n",
    "åŸºäºå…³é”®è¯å’Œè¯­ä¹‰ç›¸ä¼¼åº¦çš„å™äº‹åˆ†ç±»ï¼š\n",
    "1. **political_violence**: æ”¿æ²»æš´åŠ›å—å®³è€…å™äº‹\n",
    "2. **consequences**: è¨€è®ºåæœå™äº‹\n",
    "3. **polarization**: æ”¿æ²»æåŒ–å™äº‹\n",
    "4. **free_speech**: è¨€è®ºè‡ªç”±å™äº‹\n",
    "5. **conspiracy**: é˜´è°‹è®ºå™äº‹\n",
    "6. **memorial**: çºªå¿µä¸é—äº§å™äº‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "narrative_keywords",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– åŠ è½½è¯­ä¹‰æ¨¡å‹ç”¨äºå™äº‹æ£€æµ‹...\n",
      "ğŸ”¢ ç”Ÿæˆå™äº‹æ¡†æ¶è¯­ä¹‰å‘é‡...\n",
      "ğŸ” å¼€å§‹åŸºäºè¯­ä¹‰çš„å™äº‹æ¡†æ¶æ£€æµ‹...\n",
      "  (ä½¿ç”¨sentence embeddings + å…³é”®è¯å¢å¼º)\n",
      "\n",
      "ğŸ”¢ ç”Ÿæˆæ¨æ–‡è¯­ä¹‰å‘é‡ (4,000 æ¡)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:16<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ æ£€æµ‹å™äº‹æ¡†æ¶...\n",
      "  å¤„ç†è¿›åº¦: 2,000 / 4,000\n",
      "  å¤„ç†è¿›åº¦: 4,000 / 4,000\n",
      "\n",
      "âœ… å™äº‹æ¡†æ¶æ£€æµ‹å®Œæˆ\n",
      "\n",
      "ğŸ“Š å™äº‹åˆ†å¸ƒ:\n",
      "shape: (7, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ primary_narrative  â”† count â”‚\n",
      "â”‚ ---                â”† ---   â”‚\n",
      "â”‚ str                â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ political_violence â”† 1912  â”‚\n",
      "â”‚ memorial           â”† 1234  â”‚\n",
      "â”‚ none               â”† 485   â”‚\n",
      "â”‚ free_speech        â”† 157   â”‚\n",
      "â”‚ consequences       â”† 102   â”‚\n",
      "â”‚ conspiracy         â”† 88    â”‚\n",
      "â”‚ polarization       â”† 22    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: 0.465\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ¤– åŠ è½½è¯­ä¹‰æ¨¡å‹ç”¨äºå™äº‹æ£€æµ‹...\")\n",
    "# å¤ç”¨ä¹‹å‰çš„æ¨¡å‹æˆ–åŠ è½½è½»é‡çº§æ¨¡å‹\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# å®šä¹‰6å¤§å™äº‹æ¡†æ¶çš„åŸå‹æ–‡æœ¬ï¼ˆæ ¸å¿ƒè¯­ä¹‰æè¿°ï¼‰\n",
    "narrative_prototypes = {\n",
    "    'political_violence': [\n",
    "        \"This is a tragic political assassination and act of violence\",\n",
    "        \"Charlie Kirk was a victim of political violence and murder\",\n",
    "        \"The shooting was a terrible attack on a political figure\",\n",
    "        \"This assassination is an act of terror against conservatives\"\n",
    "    ],\n",
    "    'consequences': [\n",
    "        \"His hateful rhetoric had dangerous consequences\",\n",
    "        \"This is the result of divisive and toxic speech\",\n",
    "        \"He deserves blame for spreading hate and division\",\n",
    "        \"His inflammatory words caused this violence\"\n",
    "    ],\n",
    "    'polarization': [\n",
    "        \"America is deeply divided and polarized\",\n",
    "        \"This shows our country is on the brink of civil war\",\n",
    "        \"We treat each other as enemies instead of fellow citizens\",\n",
    "        \"Political tribalism is tearing our nation apart\"\n",
    "    ],\n",
    "    'free_speech': [\n",
    "        \"This is an attack on free speech and open debate\",\n",
    "        \"They are trying to silence conservative voices\",\n",
    "        \"We must defend the right to express political views\",\n",
    "        \"Censorship and suppression of speech led to this\"\n",
    "    ],\n",
    "    'conspiracy': [\n",
    "        \"This was a false flag operation and setup\",\n",
    "        \"The deep state planned this assassination\",\n",
    "        \"This is a psyop to manipulate public opinion\",\n",
    "        \"The official story is fake and a coverup\"\n",
    "    ],\n",
    "    'memorial': [\n",
    "        \"We honor and remember Charlie Kirk's legacy\",\n",
    "        \"His impact on conservative youth will not be forgotten\",\n",
    "        \"Rest in peace, he made a difference in politics\",\n",
    "        \"We pay tribute to his memory and contributions\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ç”Ÿæˆå™äº‹åŸå‹çš„embeddingsï¼ˆæ¯ä¸ªå™äº‹ç”¨å…¶åŸå‹æ–‡æœ¬çš„å¹³å‡embeddingï¼‰\n",
    "print(\"ğŸ”¢ ç”Ÿæˆå™äº‹æ¡†æ¶è¯­ä¹‰å‘é‡...\")\n",
    "narrative_embeddings = {}\n",
    "for narrative, prototype_texts in narrative_prototypes.items():\n",
    "    proto_embs = semantic_model.encode(prototype_texts)\n",
    "    # ä½¿ç”¨å¹³å‡å‘é‡ä½œä¸ºè¯¥å™äº‹çš„ä»£è¡¨\n",
    "    narrative_embeddings[narrative] = np.mean(proto_embs, axis=0)\n",
    "\n",
    "# å…³é”®è¯è¾…åŠ©ï¼ˆç”¨äºå¢å¼ºconfidenceï¼‰\n",
    "narrative_keywords = {\n",
    "    'political_violence': [\n",
    "        r'\\bvictim\\b', r'\\btragedy\\b', r'\\bassassinat\\w*\\b', r'\\bviolence\\b', \n",
    "        r'\\bmurder\\w*\\b', r'\\bkill\\w*\\b', r'\\bshot\\b', r'\\bshooting\\b',\n",
    "        r'\\bterror\\w*\\b', r'\\bgunman\\b', r'\\battack\\w*\\b'\n",
    "    ],\n",
    "    'consequences': [\n",
    "        r'\\brhetoric\\b', r'\\bconsequences\\b', r'\\bhate speech\\b', r'\\bdivisive\\b',\n",
    "        r'\\bresponsib\\w*\\b', r'\\bblame\\b', r'\\bcaused\\b', r'\\bdeserve\\w*\\b',\n",
    "        r'\\bkarma\\b', r'\\breap\\w*\\b'\n",
    "    ],\n",
    "    'polarization': [\n",
    "        r'\\bdivided\\b', r'\\bpolari\\w*\\b', r'\\bcivil war\\b', r'\\benemy\\b',\n",
    "        r'\\bus vs them\\b', r'\\btear\\w* apart\\b', r'\\bpartisan\\b'\n",
    "    ],\n",
    "    'free_speech': [\n",
    "        r'\\bfree speech\\b', r'\\bsilenc\\w*\\b', r'\\bcensor\\w*\\b', r'\\bdebate\\b',\n",
    "        r'\\bfirst amendment\\b', r'\\bvoice\\b', r'\\bspeak\\w* out\\b'\n",
    "    ],\n",
    "    'conspiracy': [\n",
    "        r'\\bfalse flag\\b', r'\\bsetup\\b', r'\\bdeep state\\b', r'\\bpsyop\\b',\n",
    "        r'\\bcoverup\\b', r'\\bcover-up\\b', r'\\bplanned\\b', r'\\binside job\\b',\n",
    "        r'\\bfake\\b', r'\\bhoax\\b'\n",
    "    ],\n",
    "    'memorial': [\n",
    "        r'\\blegacy\\b', r'\\bremember\\b', r'\\bhonor\\b', r'\\bimpact\\b',\n",
    "        r'\\bRIP\\b', r'\\brest in peace\\b', r'\\bmemory\\b', r'\\bmemorial\\b',\n",
    "        r'\\btribute\\b', r'\\bmiss\\w*\\b'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def detect_narratives_semantic(text, text_embedding):\n",
    "    \"\"\"åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ + å…³é”®è¯å¢å¼ºçš„å™äº‹æ£€æµ‹\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    narrative_scores = {}\n",
    "    \n",
    "    for narrative in narrative_prototypes.keys():\n",
    "        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆä¸»è¦ï¼‰\n",
    "        similarity = cosine_similarity(\n",
    "            text_embedding.reshape(1, -1),\n",
    "            narrative_embeddings[narrative].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆè¾…åŠ©å¢å¼ºï¼‰\n",
    "        keyword_matches = sum(1 for pattern in narrative_keywords[narrative] \n",
    "                             if re.search(pattern, text_lower))\n",
    "        keyword_boost = keyword_matches * 0.05  # æ¯ä¸ªå…³é”®è¯å¢åŠ 5%\n",
    "        \n",
    "        # ç»¼åˆåˆ†æ•°ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ä¸ºä¸»ï¼Œå…³é”®è¯æä¾›boost\n",
    "        final_score = similarity + keyword_boost\n",
    "        narrative_scores[narrative] = final_score\n",
    "    \n",
    "    return narrative_scores\n",
    "\n",
    "print(\"ğŸ” å¼€å§‹åŸºäºè¯­ä¹‰çš„å™äº‹æ¡†æ¶æ£€æµ‹...\")\n",
    "print(\"  (ä½¿ç”¨sentence embeddings + å…³é”®è¯å¢å¼º)\")\n",
    "\n",
    "# ç”Ÿæˆæ‰€æœ‰æ¨æ–‡çš„embeddingsï¼ˆæ‰¹é‡å¤„ç†ï¼‰\n",
    "print(f\"\\nğŸ”¢ ç”Ÿæˆæ¨æ–‡è¯­ä¹‰å‘é‡ ({len(texts):,} æ¡)...\")\n",
    "tweet_embeddings = semantic_model.encode(texts, show_progress_bar=True, batch_size=128)\n",
    "\n",
    "# å¯¹æ¯æ¡æ¨æ–‡è¿›è¡Œå™äº‹æ£€æµ‹\n",
    "print(\"\\nğŸ¯ æ£€æµ‹å™äº‹æ¡†æ¶...\")\n",
    "narrative_results = []\n",
    "for i, (text, embedding) in enumerate(zip(texts, tweet_embeddings)):\n",
    "    scores = detect_narratives_semantic(text, embedding)\n",
    "    narrative_results.append(scores)\n",
    "    \n",
    "    if (i + 1) % 2000 == 0:\n",
    "        print(f\"  å¤„ç†è¿›åº¦: {i + 1:,} / {len(texts):,}\")\n",
    "\n",
    "# æå–ä¸»å¯¼å™äº‹ï¼ˆå¾—åˆ†æœ€é«˜çš„ï¼Œä¸”é«˜äºé˜ˆå€¼0.3ï¼‰\n",
    "primary_narratives = []\n",
    "narrative_confidences = []\n",
    "for scores in narrative_results:\n",
    "    max_narrative = max(scores, key=scores.get)\n",
    "    max_score = scores[max_narrative]\n",
    "    \n",
    "    if max_score > 0.3:  # ç½®ä¿¡åº¦é˜ˆå€¼\n",
    "        primary_narratives.append(max_narrative)\n",
    "        narrative_confidences.append(max_score)\n",
    "    else:\n",
    "        primary_narratives.append('none')  # æ— æ˜æ˜¾å™äº‹\n",
    "        narrative_confidences.append(0.0)\n",
    "\n",
    "# æ·»åŠ å™äº‹åˆ†æ•°åˆ—\n",
    "narrative_cols = {}\n",
    "for narrative in narrative_prototypes.keys():\n",
    "    narrative_cols[f'narrative_{narrative}'] = [r[narrative] for r in narrative_results]\n",
    "\n",
    "df_sample = df_sample.with_columns([\n",
    "    pl.Series('primary_narrative', primary_narratives),\n",
    "    pl.Series('narrative_confidence', narrative_confidences),\n",
    "    *[pl.Series(k, v) for k, v in narrative_cols.items()]\n",
    "])\n",
    "\n",
    "print(f\"\\nâœ… å™äº‹æ¡†æ¶æ£€æµ‹å®Œæˆ\")\n",
    "print(f\"\\nğŸ“Š å™äº‹åˆ†å¸ƒ:\")\n",
    "print(df_sample.group_by('primary_narrative').agg(pl.len().alias('count')).sort('count', descending=True))\n",
    "\n",
    "print(f\"\\nğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: {np.mean([c for c in narrative_confidences if c > 0]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stance_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 4: æ”¿æ²»ç«‹åœºåˆ†ç±»ï¼ˆä¼˜åŒ–å‡çº§ï¼‰\n",
    "\n",
    "**ã€ä¼˜åŒ–æ–°å¢ã€‘æ··åˆç«‹åœºåˆ†ç±»ç­–ç•¥**:\n",
    "1. **ä½œè€…bioä¿¡å·** (author_stance_prelabel) - ä»ä½œè€…å†å²ç«‹åœºæ¨æ–­\n",
    "2. **æ¨æ–‡å…³é”®è¯åŒ¹é…** - æ¨æ–‡å†…å®¹çš„ç«‹åœºçº¿ç´¢\n",
    "3. **æƒ…æ„Ÿ-å™äº‹è”åˆæ¨ç†** - é€šè¿‡æƒ…æ„Ÿå’Œå™äº‹çš„ç»„åˆæ¨æ–­ç«‹åœº\n",
    "\n",
    "**æœ€ç»ˆå†³ç­–**: ä¸‰é‡ä¿¡å·åŠ æƒèåˆï¼Œæé«˜åˆ†ç±»å‡†ç¡®æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stance_detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å¼€å§‹æ··åˆç«‹åœºåˆ†ç±»...\n",
      "  ä¿¡å·1: ä½œè€…bioé¢„æ ‡æ³¨\n",
      "  ä¿¡å·2: æ¨æ–‡å…³é”®è¯åŒ¹é…\n",
      "  ä¿¡å·3: æƒ…æ„Ÿ-å™äº‹è”åˆæ¨ç†\n",
      "âœ… æ£€æµ‹åˆ°ä½œè€…bioé¢„æ ‡æ³¨å­—æ®µ\n",
      "\n",
      "âœ… æ··åˆç«‹åœºåˆ†ç±»å®Œæˆ\n",
      "\n",
      "ğŸ“Š ã€ä¼˜åŒ–åã€‘ç«‹åœºåˆ†å¸ƒ:\n",
      "shape: (3, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ political_stance â”† count â”‚\n",
      "â”‚ ---              â”† ---   â”‚\n",
      "â”‚ str              â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ neutral          â”† 3930  â”‚\n",
      "â”‚ liberal          â”† 46    â”‚\n",
      "â”‚ conservative     â”† 24    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“Š ã€å¯¹æ¯”ã€‘åŸçº¯å…³é”®è¯æ–¹æ³•çš„åˆ†å¸ƒ:\n",
      "shape: (3, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ political_stance â”† count â”‚\n",
      "â”‚ ---              â”† ---   â”‚\n",
      "â”‚ str              â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ neutral          â”† 3624  â”‚\n",
      "â”‚ liberal          â”† 243   â”‚\n",
      "â”‚ conservative     â”† 133   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ¯ å…³é”®æ”¹è¿›:\n",
      "  ä¸­ç«‹æ¯”ä¾‹: 90.6% â†’ 98.2%\n",
      "  æœ‰ç«‹åœºæ¨æ–‡: 376 â†’ 70 (+-306)\n"
     ]
    }
   ],
   "source": [
    "# ã€ä¼˜åŒ–å‡çº§ã€‘æ··åˆç«‹åœºåˆ†ç±»å™¨\n",
    "\n",
    "# 1. æ¨æ–‡å…³é”®è¯ä¿¡å·ï¼ˆä¿ç•™åŸæœ‰å…³é”®è¯ï¼Œæ‰©å±•è¦†ç›–ï¼‰\n",
    "stance_keywords = {\n",
    "    'conservative': [\n",
    "        r'\\bhero\\b', r'\\bpatriot\\b', r'\\bfreedom fighter\\b', r'\\bdefend\\w*\\b',\n",
    "        r'\\bMAGA\\b', r'\\bTrump\\b', r'\\bconservative movement\\b',\n",
    "        r'\\bleft\\w* violence\\b', r'\\bsocialist\\w*\\b', r'\\bliberal\\w* violence\\b',\n",
    "        r'\\bmarty\\w*\\b', r'\\bstanding up\\b', r'\\bpray\\w* for\\b.*\\bfamily\\b',\n",
    "        r'\\bRIP\\b.*\\blegend\\b', r'\\bAmerica First\\b'\n",
    "    ],\n",
    "    'liberal': [\n",
    "        r'\\bhateful\\b', r'\\btoxic\\b', r'\\bdangerous rhetoric\\b',\n",
    "        r'\\bextremis\\w*\\b', r'\\bhate speech\\b', r'\\bconsequences\\b',\n",
    "        r'\\bdeserve\\w*\\b', r'\\breap what\\b', r'\\bfar-right\\b',\n",
    "        r'\\bTurning Point\\b.*\\bnegative\\b', r'\\bkarma\\b', r'\\bfinally\\b',\n",
    "        r'\\bspread\\w* hate\\b', r'\\bincit\\w*\\b'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def detect_stance_from_text(text: str) -> tuple[str, float]:\n",
    "    \"\"\"åŸºäºæ¨æ–‡å…³é”®è¯æ£€æµ‹ç«‹åœº\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    conservative_score = sum(1 for pattern in stance_keywords['conservative'] if re.search(pattern, text_lower))\n",
    "    liberal_score = sum(1 for pattern in stance_keywords['liberal'] if re.search(pattern, text_lower))\n",
    "    \n",
    "    if conservative_score > liberal_score and conservative_score > 0:\n",
    "        return 'conservative', min(conservative_score * 0.3, 1.0)\n",
    "    elif liberal_score > conservative_score and liberal_score > 0:\n",
    "        return 'liberal', min(liberal_score * 0.3, 1.0)\n",
    "    else:\n",
    "        return 'neutral', 0.0\n",
    "\n",
    "def infer_stance_from_emotion_narrative(emotion: str, narrative: str, \n",
    "                                       emotion_anger: float, emotion_sadness: float) -> tuple[str, float]:\n",
    "    \"\"\"åŸºäºæƒ…æ„Ÿ-å™äº‹ç»„åˆæ¨æ–­ç«‹åœº\"\"\"\n",
    "    # è§„åˆ™1: æ‰¹åˆ¤æ€§å™äº‹ + æ„¤æ€’ â†’ è‡ªç”±æ´¾\n",
    "    if narrative == 'consequences' and emotion_anger > 0.25:\n",
    "        return 'liberal', 0.4\n",
    "    \n",
    "    # è§„åˆ™2: æš´åŠ›å—å®³è€…å™äº‹ + æ‚²ä¼¤ â†’ ä¿å®ˆæ´¾\n",
    "    if narrative == 'political_violence' and emotion_sadness > 0.2:\n",
    "        return 'conservative', 0.4\n",
    "    \n",
    "    # è§„åˆ™3: çºªå¿µå™äº‹ + æ‚²ä¼¤ â†’ ä¿å®ˆæ´¾å€¾å‘\n",
    "    if narrative == 'memorial' and emotion_sadness > 0.15:\n",
    "        return 'conservative', 0.3\n",
    "    \n",
    "    # è§„åˆ™4: è¨€è®ºè‡ªç”±å™äº‹ â†’ ä¿å®ˆæ´¾å€¾å‘\n",
    "    if narrative == 'free_speech':\n",
    "        return 'conservative', 0.35\n",
    "    \n",
    "    # è§„åˆ™5: é˜´è°‹è®º + æ„¤æ€’ â†’ ä¿å®ˆæ´¾æç«¯\n",
    "    if narrative == 'conspiracy' and emotion_anger > 0.2:\n",
    "        return 'conservative', 0.3\n",
    "    \n",
    "    return 'neutral', 0.0\n",
    "\n",
    "def fuse_stance_signals(author_stance: str, author_conf: float,\n",
    "                       text_stance: str, text_conf: float,\n",
    "                       emotion_stance: str, emotion_conf: float) -> tuple[str, float]:\n",
    "    \"\"\"èåˆä¸‰é‡ç«‹åœºä¿¡å·ï¼ŒåŠ æƒæŠ•ç¥¨\"\"\"\n",
    "    # æƒé‡è®¾ç½®ï¼šä½œè€…bio > æ¨æ–‡å…³é”®è¯ > æƒ…æ„Ÿå™äº‹\n",
    "    weights = {\n",
    "        'author': 0.5,   # ä½œè€…å†å²ç«‹åœºæœ€å¯é \n",
    "        'text': 0.35,    # æ¨æ–‡å†…å®¹æ¬¡ä¹‹\n",
    "        'emotion': 0.15  # æƒ…æ„Ÿå™äº‹è¾…åŠ©\n",
    "    }\n",
    "    \n",
    "    # è®¡ç®—åŠ æƒå¾—åˆ†\n",
    "    scores = {'conservative': 0.0, 'liberal': 0.0, 'neutral': 0.0}\n",
    "    \n",
    "    # ä½œè€…ä¿¡å·\n",
    "    if author_stance != 'neutral' and author_conf > 0:\n",
    "        scores[author_stance] += weights['author'] * author_conf\n",
    "    \n",
    "    # æ¨æ–‡ä¿¡å·\n",
    "    if text_stance != 'neutral' and text_conf > 0:\n",
    "        scores[text_stance] += weights['text'] * text_conf\n",
    "    \n",
    "    # æƒ…æ„Ÿå™äº‹ä¿¡å·\n",
    "    if emotion_stance != 'neutral' and emotion_conf > 0:\n",
    "        scores[emotion_stance] += weights['emotion'] * emotion_conf\n",
    "    \n",
    "    # å†³ç­–ï¼šå–æœ€é«˜åˆ†ï¼Œéœ€è¶…è¿‡é˜ˆå€¼0.15\n",
    "    max_stance = max(scores, key=scores.get)\n",
    "    max_score = scores[max_stance]\n",
    "    \n",
    "    if max_score > 0.15:\n",
    "        return max_stance, min(max_score, 1.0)\n",
    "    else:\n",
    "        return 'neutral', 0.0\n",
    "\n",
    "print(\"ğŸ¯ å¼€å§‹æ··åˆç«‹åœºåˆ†ç±»...\")\n",
    "print(\"  ä¿¡å·1: ä½œè€…bioé¢„æ ‡æ³¨\")\n",
    "print(\"  ä¿¡å·2: æ¨æ–‡å…³é”®è¯åŒ¹é…\")\n",
    "print(\"  ä¿¡å·3: æƒ…æ„Ÿ-å™äº‹è”åˆæ¨ç†\")\n",
    "\n",
    "# æå–æ‰€éœ€å­—æ®µ\n",
    "texts = df_sample['text'].to_list()\n",
    "\n",
    "# ã€é˜²å¾¡æ€§æ£€æŸ¥ã€‘æ£€æŸ¥æ–°å¢å­—æ®µæ˜¯å¦å­˜åœ¨\n",
    "if 'author_stance_prelabel' in df_sample.columns and 'author_stance_confidence' in df_sample.columns:\n",
    "    author_stances = df_sample['author_stance_prelabel'].to_list()\n",
    "    author_confs = df_sample['author_stance_confidence'].to_list()\n",
    "    print(\"âœ… æ£€æµ‹åˆ°ä½œè€…bioé¢„æ ‡æ³¨å­—æ®µ\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° author_stance_prelabel å’Œ author_stance_confidence å­—æ®µ\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nğŸ“‹ åŸå› åˆ†æ:\")\n",
    "    print(\"   tweets_enriched.parquet æ–‡ä»¶æœªåŒ…å«æ–°å¢å­—æ®µ\")\n",
    "    print(\"\\nğŸ”§ è§£å†³æ–¹æ¡ˆ:\")\n",
    "    print(\"   1. ç¡®è®¤å·²è¿è¡Œæœ€æ–°ç‰ˆæœ¬çš„ 00_data_intake.ipynb\")\n",
    "    print(\"   2. æ£€æŸ¥ 00_data_intake.ipynb Step 6 æ˜¯å¦æˆåŠŸæ‰§è¡Œï¼ˆåº”è¯¥æ˜¾ç¤ºæ–°å­—æ®µï¼‰\")\n",
    "    print(\"   3. éªŒè¯ parquet æ–‡ä»¶:\")\n",
    "    print(\"      import polars as pl\")\n",
    "    print(\"      df = pl.read_parquet('../parquet/tweets_enriched.parquet')\")\n",
    "    print(\"      print(df.columns)  # æ£€æŸ¥æ˜¯å¦åŒ…å« author_stance_prelabel\")\n",
    "    print(\"   4. å¦‚æœå­—æ®µå­˜åœ¨ä½†ä»æŠ¥é”™ï¼Œé‡å¯ Jupyter kernel åé‡æ–°è¿è¡Œ\")\n",
    "    print(\"\\nâš™ï¸  å½“å‰é™çº§æ¨¡å¼:\")\n",
    "    print(\"   ä½¿ç”¨ neutral ä½œä¸ºé»˜è®¤ä½œè€…ç«‹åœºï¼Œä»…å¯ç”¨ä¿¡å·2å’Œä¿¡å·3\")\n",
    "    print(\"   åˆ†ç±»å‡†ç¡®æ€§ä¼šä¸‹é™ï¼Œå»ºè®®ä¿®å¤åé‡æ–°è¿è¡Œ\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    author_stances = ['neutral'] * len(texts)\n",
    "    author_confs = [0.0] * len(texts)\n",
    "\n",
    "primary_emotions = df_sample['primary_emotion'].to_list()\n",
    "primary_narratives = df_sample['primary_narrative'].to_list()\n",
    "emotion_angers = df_sample['emotion_anger'].to_list()\n",
    "emotion_sadnesses = df_sample['emotion_sadness'].to_list()\n",
    "\n",
    "# å¯¹æ¯æ¡æ¨æ–‡è¿›è¡Œä¸‰é‡ä¿¡å·èåˆ\n",
    "final_stances = []\n",
    "final_confidences = []\n",
    "signal_details = []  # ç”¨äºè°ƒè¯•å’ŒéªŒè¯\n",
    "\n",
    "for i, (text, author_st, author_cf, emotion, narrative, anger, sadness) in enumerate(zip(\n",
    "    texts, author_stances, author_confs, primary_emotions, primary_narratives, \n",
    "    emotion_angers, emotion_sadnesses\n",
    ")):\n",
    "    # ä¿¡å·1: ä½œè€…bio\n",
    "    # å¤„ç†ç©ºå€¼ï¼šå¦‚æœauthor_stance_prelabelæ˜¯Noneï¼Œè®¾ä¸º'neutral'\n",
    "    author_st = author_st if author_st is not None else 'neutral'\n",
    "    author_cf = author_cf if author_cf is not None else 0.0\n",
    "    \n",
    "    # ä¿¡å·2: æ¨æ–‡å…³é”®è¯\n",
    "    text_st, text_cf = detect_stance_from_text(text)\n",
    "    \n",
    "    # ä¿¡å·3: æƒ…æ„Ÿ-å™äº‹\n",
    "    emotion_st, emotion_cf = infer_stance_from_emotion_narrative(emotion, narrative, anger, sadness)\n",
    "    \n",
    "    # èåˆå†³ç­–\n",
    "    final_st, final_cf = fuse_stance_signals(\n",
    "        author_st, author_cf,\n",
    "        text_st, text_cf,\n",
    "        emotion_st, emotion_cf\n",
    "    )\n",
    "    \n",
    "    final_stances.append(final_st)\n",
    "    final_confidences.append(final_cf)\n",
    "    signal_details.append({\n",
    "        'author': (author_st, author_cf),\n",
    "        'text': (text_st, text_cf),\n",
    "        'emotion': (emotion_st, emotion_cf)\n",
    "    })\n",
    "    \n",
    "    if (i + 1) % 5000 == 0:\n",
    "        print(f\"  å¤„ç†è¿›åº¦: {i + 1:,} / {len(texts):,}\")\n",
    "\n",
    "# æ·»åŠ åˆ°dataframe\n",
    "df_sample = df_sample.with_columns([\n",
    "    pl.Series('political_stance', final_stances),\n",
    "    pl.Series('stance_confidence', final_confidences)\n",
    "])\n",
    "\n",
    "print(f\"\\nâœ… æ··åˆç«‹åœºåˆ†ç±»å®Œæˆ\")\n",
    "print(f\"\\nğŸ“Š ã€ä¼˜åŒ–åã€‘ç«‹åœºåˆ†å¸ƒ:\")\n",
    "new_dist = df_sample.group_by('political_stance').agg(pl.len().alias('count')).sort('count', descending=True)\n",
    "print(new_dist)\n",
    "\n",
    "# å¯¹æ¯”åŸæ–¹æ³•çš„ç»“æœï¼ˆä»…ç”¨å…³é”®è¯ï¼‰\n",
    "print(f\"\\nğŸ“Š ã€å¯¹æ¯”ã€‘åŸçº¯å…³é”®è¯æ–¹æ³•çš„åˆ†å¸ƒ:\")\n",
    "old_stances = [detect_stance_from_text(t)[0] for t in texts]\n",
    "old_dist = pl.DataFrame({'political_stance': old_stances}).group_by('political_stance').agg(pl.len().alias('count')).sort('count', descending=True)\n",
    "print(old_dist)\n",
    "\n",
    "print(f\"\\nğŸ¯ å…³é”®æ”¹è¿›:\")\n",
    "neutral_old = old_dist.filter(pl.col('political_stance') == 'neutral')['count'][0] if old_dist.filter(pl.col('political_stance') == 'neutral').height > 0 else 0\n",
    "neutral_new = new_dist.filter(pl.col('political_stance') == 'neutral')['count'][0] if new_dist.filter(pl.col('political_stance') == 'neutral').height > 0 else 0\n",
    "print(f\"  ä¸­ç«‹æ¯”ä¾‹: {neutral_old/len(texts)*100:.1f}% â†’ {neutral_new/len(texts)*100:.1f}%\")\n",
    "print(f\"  æœ‰ç«‹åœºæ¨æ–‡: {len(texts) - neutral_old} â†’ {len(texts) - neutral_new} (+{len(texts) - neutral_new - (len(texts) - neutral_old)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 5: æ—¶é—´æ¼”å˜åˆ†æ\n",
    "\n",
    "åˆ†ææƒ…æ„Ÿä¸å™äº‹åœ¨5ä¸ªæ—¶é—´çª—å£çš„æ¼”å˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "temporal_evolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ æ—¶é—´æ¼”å˜åˆ†æ\n",
      "\n",
      "ğŸ­ æƒ…æ„Ÿæ¼”å˜ (å¹³å‡åˆ†æ•°):\n",
      "shape: (2, 8)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ time_window â”† tweet_count â”† avg_sadnes â”† avg_anger â”† avg_fear â”† avg_surpri â”† avg_joy  â”† avg_love â”‚\n",
      "â”‚ ---         â”† ---         â”† s          â”† ---       â”† ---      â”† se         â”† ---      â”† ---      â”‚\n",
      "â”‚ str         â”† u32         â”† ---        â”† f64       â”† f64      â”† ---        â”† f64      â”† f64      â”‚\n",
      "â”‚             â”†             â”† f64        â”†           â”†          â”† f64        â”†          â”†          â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 24-48h      â”† 2000        â”† 0.141026   â”† 0.233624  â”† 0.214377 â”† 0.106929   â”† 0.069154 â”† 0.0      â”‚\n",
      "â”‚ 48-72h      â”† 2000        â”† 0.144883   â”† 0.25309   â”† 0.176424 â”† 0.121406   â”† 0.062338 â”† 0.0      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“– å™äº‹æ¼”å˜ (å„æ—¶æ®µtop3å™äº‹):\n",
      "\n",
      "  0-6h:\n",
      "\n",
      "  6-12h:\n",
      "\n",
      "  12-24h:\n",
      "\n",
      "  24-48h:\n",
      "    - political_violence: 992 æ¡\n",
      "    - memorial: 615 æ¡\n",
      "    - none: 222 æ¡\n",
      "\n",
      "  48-72h:\n",
      "    - political_violence: 920 æ¡\n",
      "    - memorial: 619 æ¡\n",
      "    - none: 263 æ¡\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“ˆ æ—¶é—´æ¼”å˜åˆ†æ\")\n",
    "\n",
    "# æƒ…æ„Ÿéšæ—¶é—´æ¼”å˜\n",
    "emotion_evolution = df_sample.group_by('time_window').agg([\n",
    "    pl.len().alias('tweet_count'),\n",
    "    pl.col('emotion_sadness').mean().alias('avg_sadness'),\n",
    "    pl.col('emotion_anger').mean().alias('avg_anger'),\n",
    "    pl.col('emotion_fear').mean().alias('avg_fear'),\n",
    "    pl.col('emotion_surprise').mean().alias('avg_surprise'),\n",
    "    pl.col('emotion_joy').mean().alias('avg_joy'),\n",
    "    pl.col('emotion_love').mean().alias('avg_love')\n",
    "]).sort('time_window')\n",
    "\n",
    "print(\"\\nğŸ­ æƒ…æ„Ÿæ¼”å˜ (å¹³å‡åˆ†æ•°):\")\n",
    "print(emotion_evolution)\n",
    "\n",
    "# å™äº‹éšæ—¶é—´æ¼”å˜\n",
    "narrative_evolution = df_sample.group_by(['time_window', 'primary_narrative']).agg(\n",
    "    pl.len().alias('count')\n",
    ").sort(['time_window', 'count'], descending=[False, True])\n",
    "\n",
    "print(\"\\nğŸ“– å™äº‹æ¼”å˜ (å„æ—¶æ®µtop3å™äº‹):\")\n",
    "for window in ['0-6h', '6-12h', '12-24h', '24-48h', '48-72h']:\n",
    "    top_narratives = narrative_evolution.filter(pl.col('time_window') == window).head(3)\n",
    "    print(f\"\\n  {window}:\")\n",
    "    for row in top_narratives.iter_rows(named=True):\n",
    "        print(f\"    - {row['primary_narrative']}: {row['count']} æ¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 6: æå–ä»£è¡¨æ€§æ¨æ–‡\n",
    "\n",
    "æ¯ç±»å™äº‹é€‰æ‹©2æ¡æœ€å…·ä»£è¡¨æ€§çš„æ¨æ–‡ï¼ˆåŸºäºengagementå’Œå™äº‹å¾—åˆ†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "representative_tweets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æå–ä»£è¡¨æ€§æ¨æ–‡...\n",
      "\n",
      "âœ… ä»£è¡¨æ€§æ¨æ–‡æå–å®Œæˆ\n",
      "\n",
      "ğŸ† å„å™äº‹ä»£è¡¨æ€§æ¨æ–‡ï¼ˆå‰100å­—ç¬¦ï¼‰:\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“ æå–ä»£è¡¨æ€§æ¨æ–‡...\")\n",
    "\n",
    "# è®¡ç®—engagementåˆ†æ•°\n",
    "df_sample = df_sample.with_columns(\n",
    "    (pl.col('retweetCount') + pl.col('likeCount') * 0.5 + pl.col('replyCount') * 0.3).alias('engagement_score')\n",
    ")\n",
    "\n",
    "representative_tweets = {}\n",
    "\n",
    "for narrative in narrative_keywords.keys():\n",
    "    # ç­›é€‰è¯¥å™äº‹çš„æ¨æ–‡\n",
    "    narrative_tweets = df_sample.filter(\n",
    "        (pl.col('primary_narrative') == narrative) &\n",
    "        (pl.col(f'narrative_{narrative}') >= 2)  # è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯\n",
    "    ).sort('engagement_score', descending=True).head(2)\n",
    "    \n",
    "    if narrative_tweets.height > 0:\n",
    "        representative_tweets[narrative] = narrative_tweets.select(['text', 'engagement_score', 'primary_emotion']).to_dicts()\n",
    "\n",
    "print(f\"\\nâœ… ä»£è¡¨æ€§æ¨æ–‡æå–å®Œæˆ\")\n",
    "print(f\"\\nğŸ† å„å™äº‹ä»£è¡¨æ€§æ¨æ–‡ï¼ˆå‰100å­—ç¬¦ï¼‰:\")\n",
    "for narrative, tweets in representative_tweets.items():\n",
    "    print(f\"\\nã€{narrative.upper()}ã€‘\")\n",
    "    for i, tweet in enumerate(tweets, 1):\n",
    "        print(f\"  {i}. [{tweet['primary_emotion']}] {tweet['text'][:100]}...\")\n",
    "        print(f\"     Engagement: {tweet['engagement_score']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 7: ä¿å­˜åˆ†æç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "save_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å†…å®¹åˆ†æç»“æœå·²ä¿å­˜: ../parquet/content_analysis.parquet\n",
      "âœ… æƒ…æ„Ÿæ¼”å˜æ•°æ®å·²ä¿å­˜: ../parquet/emotion_evolution.parquet\n",
      "âœ… å™äº‹æ¼”å˜æ•°æ®å·²ä¿å­˜: ../parquet/narrative_evolution.parquet\n",
      "\n",
      "ğŸ“Š æ•°æ®æ¦‚è§ˆ:\n",
      "  æ€»åˆ†ææ¨æ–‡æ•°: 4,000\n",
      "  æ—¶é—´çª—å£æ•°: 5\n",
      "  æƒ…æ„Ÿç»´åº¦: 6\n",
      "  å™äº‹æ¡†æ¶: 6\n",
      "  æ”¿æ²»ç«‹åœº: 3\n"
     ]
    }
   ],
   "source": [
    "from src import io\n",
    "\n",
    "# ä¿å­˜å®Œæ•´çš„å†…å®¹åˆ†ææ•°æ®\n",
    "content_path = Path(\"../parquet/content_analysis.parquet\")\n",
    "io.materialize_parquet(df_sample.lazy(), content_path)\n",
    "print(f\"âœ… å†…å®¹åˆ†æç»“æœå·²ä¿å­˜: {content_path}\")\n",
    "\n",
    "# ä¿å­˜æƒ…æ„Ÿæ¼”å˜æ•°æ®\n",
    "emotion_evo_path = Path(\"../parquet/emotion_evolution.parquet\")\n",
    "io.materialize_parquet(emotion_evolution.lazy(), emotion_evo_path)\n",
    "print(f\"âœ… æƒ…æ„Ÿæ¼”å˜æ•°æ®å·²ä¿å­˜: {emotion_evo_path}\")\n",
    "\n",
    "# ä¿å­˜å™äº‹æ¼”å˜æ•°æ®\n",
    "narrative_evo_path = Path(\"../parquet/narrative_evolution.parquet\")\n",
    "io.materialize_parquet(narrative_evolution.lazy(), narrative_evo_path)\n",
    "print(f\"âœ… å™äº‹æ¼”å˜æ•°æ®å·²ä¿å­˜: {narrative_evo_path}\")\n",
    "\n",
    "# ä¿å­˜ä»£è¡¨æ€§æ¨æ–‡ï¼ˆè½¬ä¸ºDataFrameï¼‰\n",
    "repr_tweets_list = []\n",
    "for narrative, tweets in representative_tweets.items():\n",
    "    for tweet in tweets:\n",
    "        repr_tweets_list.append({\n",
    "            'narrative': narrative,\n",
    "            'text': tweet['text'],\n",
    "            'emotion': tweet['primary_emotion'],\n",
    "            'engagement': tweet['engagement_score']\n",
    "        })\n",
    "\n",
    "if repr_tweets_list:\n",
    "    repr_tweets_df = pl.DataFrame(repr_tweets_list)\n",
    "    repr_tweets_path = Path(\"../parquet/representative_tweets.parquet\")\n",
    "    io.materialize_parquet(repr_tweets_df.lazy(), repr_tweets_path)\n",
    "    print(f\"âœ… ä»£è¡¨æ€§æ¨æ–‡å·²ä¿å­˜: {repr_tweets_path}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "print(f\"  æ€»åˆ†ææ¨æ–‡æ•°: {df_sample.height:,}\")\n",
    "print(f\"  æ—¶é—´çª—å£æ•°: 5\")\n",
    "print(f\"  æƒ…æ„Ÿç»´åº¦: 6\")\n",
    "print(f\"  å™äº‹æ¡†æ¶: 6\")\n",
    "print(f\"  æ”¿æ²»ç«‹åœº: 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## âœ… å†…å®¹åˆ†æå®Œæˆï¼\n",
    "\n",
    "**ç”Ÿæˆçš„æ ¸å¿ƒæ•°æ®**:\n",
    "- `content_analysis.parquet`: å®Œæ•´çš„æƒ…æ„Ÿã€å™äº‹ã€ç«‹åœºåˆ†æç»“æœ\n",
    "- `emotion_evolution.parquet`: 6å¤§æƒ…æ„Ÿéšæ—¶é—´çš„æ¼”å˜\n",
    "- `narrative_evolution.parquet`: 6å¤§å™äº‹éšæ—¶é—´çš„æ¼”å˜\n",
    "- `representative_tweets.parquet`: å„å™äº‹çš„ä»£è¡¨æ€§æ¨æ–‡\n",
    "\n",
    "**ã€ä¼˜åŒ–æ–°å¢ã€‘ç«‹åœºåˆ†ç±»æ”¹è¿›**:\n",
    "- âœ… æ··åˆåˆ†ç±»å™¨ï¼šä½œè€…bio (50%) + æ¨æ–‡å…³é”®è¯ (35%) + æƒ…æ„Ÿå™äº‹ (15%)\n",
    "- âœ… æ›´å‡†ç¡®çš„ç«‹åœºè¯†åˆ«ï¼Œå‡å°‘è¯¯åˆ¤ä¸ºä¸­ç«‹çš„æ¯”ä¾‹\n",
    "- âœ… ä¿æŒå‘åå…¼å®¹ï¼šè¾“å‡ºå­—æ®µä¸åŸç‰ˆå®Œå…¨ä¸€è‡´\n",
    "\n",
    "**ä¸‹ä¸€æ­¥**: \n",
    "1. è¿è¡Œ `02_temporal_evolution.ipynb` ç”Ÿæˆå°æ—¶çº§æ—¶é—´åºåˆ—\n",
    "2. ã€æ–°å¢ã€‘è¿è¡Œ `03_author_profiling.ipynb` åˆ†æä½œè€…ç”»åƒä¸å½±å“åŠ›\n",
    "3. æ„å»ºå¯è§†åŒ–Dashboardå±•ç¤ºæ‰€æœ‰æ´å¯Ÿ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
