{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 03 å†…å®¹åˆ†æï¼šæƒ…æ„Ÿã€å™äº‹ä¸æ”¿æ²»ç«‹åœº\n",
    "\n",
    "**ç ”ç©¶æ ¸å¿ƒ**: Charlie Kirkæ”¿æ²»æš—æ€äº‹ä»¶å72å°æ—¶çš„ç¤¾äº¤åª’ä½“èˆ†è®ºå†…å®¹åˆ†æ\n",
    "\n",
    "**åˆ†æç»´åº¦**:\n",
    "1. **6ç»´æƒ…æ„Ÿåˆ†æ**: sadness, anger, fear, surprise, disgust, joy\n",
    "2. **6å¤§å™äº‹æ¡†æ¶**: æ”¿æ²»æš´åŠ›å—å®³è€…ã€è¨€è®ºåæœã€æ”¿æ²»æåŒ–ã€è¨€è®ºè‡ªç”±ã€é˜´è°‹è®ºã€çºªå¿µé—äº§\n",
    "3. **æ”¿æ²»ç«‹åœºåˆ†ç±»**: conservative, liberal, neutral\n",
    "4. **æ—¶é—´æ¼”å˜**: æƒ…æ„Ÿä¸å™äº‹éš72å°æ—¶çš„å˜åŒ–\n",
    "5. **ä»£è¡¨æ€§å†…å®¹**: æ¯ç±»å™äº‹çš„å…¸å‹æ¨æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Python è·¯å¾„å·²é…ç½®: /workspace\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„\n",
    "project_root = Path('/workspace')\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    \n",
    "print(f\"âœ… Python è·¯å¾„å·²é…ç½®: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 1: åŠ è½½æ•°æ®å¹¶é‡‡æ ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: 508,954 è¡Œ\n",
      "ğŸ“ æœ‰æ•ˆè‹±æ–‡æ¨æ–‡: 415,714 æ¡\n",
      "  48-72h: 415,714 æ¡ â†’ é‡‡æ · 2,000 æ¡\n",
      "\n",
      "ğŸ“‹ é‡‡æ ·å®Œæˆ: 2,000 æ¡æ¨æ–‡\n",
      "\n",
      "æ—¶é—´çª—å£åˆ†å¸ƒ:\n",
      "shape: (1, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ time_window â”† count â”‚\n",
      "â”‚ ---         â”† ---   â”‚\n",
      "â”‚ str         â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ 48-72h      â”† 2000  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# åŠ è½½enrichedæ•°æ®ï¼ˆåŒ…å«event_time_delta_hourså’Œtime_windowå­—æ®µï¼‰\n",
    "df = pl.read_parquet(\"../parquet/tweets_enriched.parquet\")\n",
    "print(f\"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {df.height:,} è¡Œ\")\n",
    "\n",
    "# è¿‡æ»¤æœ‰æ•ˆè‹±æ–‡æ–‡æœ¬\n",
    "df_text = df.filter(\n",
    "    (pl.col('text').is_not_null()) & \n",
    "    (pl.col('lang') == 'en') &\n",
    "    (pl.col('text').str.len_chars() > 20)  # è‡³å°‘20å­—ç¬¦\n",
    ")\n",
    "print(f\"ğŸ“ æœ‰æ•ˆè‹±æ–‡æ¨æ–‡: {df_text.height:,} æ¡\")\n",
    "\n",
    "# é‡‡æ ·ç­–ç•¥ï¼šæ¯ä¸ªæ—¶é—´çª—å£é‡‡æ ·æœ€å¤š2000æ¡ï¼ˆç¡®ä¿æ—¶é—´æ¼”å˜åˆ†æçš„ä»£è¡¨æ€§ï¼‰\n",
    "sample_per_window = 2000\n",
    "\n",
    "# æ–¹æ³•ï¼šå¯¹æ¯ä¸ªæ—¶é—´çª—å£åˆ†åˆ«é‡‡æ ·ååˆå¹¶\n",
    "sampled_dfs = []\n",
    "for window in df_text['time_window'].unique().sort():\n",
    "    window_df = df_text.filter(pl.col('time_window') == window)\n",
    "    # å¦‚æœè¯¥çª—å£æ•°æ®å°‘äº2000ï¼Œå…¨éƒ¨ä½¿ç”¨ï¼›å¦åˆ™é‡‡æ ·2000\n",
    "    n_sample = min(sample_per_window, window_df.height)\n",
    "    sampled = window_df.sample(n=n_sample, seed=42)\n",
    "    sampled_dfs.append(sampled)\n",
    "    print(f\"  {window}: {window_df.height:,} æ¡ â†’ é‡‡æ · {n_sample:,} æ¡\")\n",
    "\n",
    "df_sample = pl.concat(sampled_dfs).sort('createdAt')\n",
    "\n",
    "print(f\"\\nğŸ“‹ é‡‡æ ·å®Œæˆ: {df_sample.height:,} æ¡æ¨æ–‡\")\n",
    "print(f\"\\næ—¶é—´çª—å£åˆ†å¸ƒ:\")\n",
    "print(df_sample.group_by('time_window').agg(pl.len().alias('count')).sort('time_window'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotion_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 2: å…­ç»´æƒ…æ„Ÿåˆ†æ\n",
    "\n",
    "ä½¿ç”¨ HuggingFace `j-hartmann/emotion-english-distilroberta-base` æ¨¡å‹  \n",
    "åˆ†ç±»ï¼šsadness, joy, love, anger, fear, surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emotion_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹åŠ è½½å®Œæˆ (device: CPU)\n",
      "\n",
      "ğŸ”„ å¼€å§‹æƒ…æ„Ÿåˆ†æ (2,000 æ¡æ¨æ–‡)...\n",
      "âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ\n",
      "\n",
      "ğŸ“Š æƒ…æ„Ÿåˆ†å¸ƒ:\n",
      "shape: (7, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ primary_emotion â”† count â”‚\n",
      "â”‚ ---             â”† ---   â”‚\n",
      "â”‚ str             â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ anger           â”† 463   â”‚\n",
      "â”‚ neutral         â”† 449   â”‚\n",
      "â”‚ fear            â”† 394   â”‚\n",
      "â”‚ sadness         â”† 268   â”‚\n",
      "â”‚ surprise        â”† 207   â”‚\n",
      "â”‚ joy             â”† 141   â”‚\n",
      "â”‚ disgust         â”† 78    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(\"ğŸ¤– åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    device=device,\n",
    "    top_k=None  # è¿”å›æ‰€æœ‰æƒ…æ„Ÿçš„æ¦‚ç‡\n",
    ")\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (device: {'GPU' if device == 0 else 'CPU'})\")\n",
    "\n",
    "# å¤„ç†æ–‡æœ¬ï¼ˆæ‰¹é‡æ¨ç†ï¼‰\n",
    "texts = df_sample['text'].to_list()\n",
    "print(f\"\\nğŸ”„ å¼€å§‹æƒ…æ„Ÿåˆ†æ ({len(texts):,} æ¡æ¨æ–‡)...\")\n",
    "\n",
    "# æ‰¹é‡å¤„ç†ï¼Œæ¯æ‰¹128æ¡\n",
    "batch_size = 128\n",
    "all_emotions = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    # æˆªæ–­é•¿æ–‡æœ¬\n",
    "    batch_truncated = [t[:512] for t in batch]\n",
    "    results = emotion_classifier(batch_truncated)\n",
    "    all_emotions.extend(results)\n",
    "    \n",
    "    if (i + batch_size) % 1000 == 0:\n",
    "        print(f\"  å¤„ç†è¿›åº¦: {i + batch_size:,} / {len(texts):,}\")\n",
    "\n",
    "print(f\"âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ\")\n",
    "\n",
    "# æå–ä¸»è¦æƒ…æ„Ÿå’Œç½®ä¿¡åº¦\n",
    "primary_emotions = [max(e, key=lambda x: x['score'])['label'] for e in all_emotions]\n",
    "primary_scores = [max(e, key=lambda x: x['score'])['score'] for e in all_emotions]\n",
    "\n",
    "# æå–6å¤§æƒ…æ„Ÿçš„åˆ†æ•°ï¼ˆæ„å»ºæƒ…æ„Ÿå‘é‡ï¼‰\n",
    "emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "emotion_vectors = {}\n",
    "for label in emotion_labels:\n",
    "    scores = []\n",
    "    for result in all_emotions:\n",
    "        score_dict = {item['label']: item['score'] for item in result}\n",
    "        scores.append(score_dict.get(label, 0.0))\n",
    "    emotion_vectors[f'emotion_{label}'] = scores\n",
    "\n",
    "# æ·»åŠ åˆ°dataframe\n",
    "df_sample = df_sample.with_columns([\n",
    "    pl.Series('primary_emotion', primary_emotions),\n",
    "    pl.Series('emotion_confidence', primary_scores),\n",
    "    *[pl.Series(k, v) for k, v in emotion_vectors.items()]\n",
    "])\n",
    "\n",
    "print(f\"\\nğŸ“Š æƒ…æ„Ÿåˆ†å¸ƒ:\")\n",
    "print(df_sample.group_by('primary_emotion').agg(pl.len().alias('count')).sort('count', descending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 3: å…­å¤§å™äº‹æ¡†æ¶æ£€æµ‹\n",
    "\n",
    "åŸºäºå…³é”®è¯å’Œè¯­ä¹‰ç›¸ä¼¼åº¦çš„å™äº‹åˆ†ç±»ï¼š\n",
    "1. **political_violence**: æ”¿æ²»æš´åŠ›å—å®³è€…å™äº‹\n",
    "2. **consequences**: è¨€è®ºåæœå™äº‹\n",
    "3. **polarization**: æ”¿æ²»æåŒ–å™äº‹\n",
    "4. **free_speech**: è¨€è®ºè‡ªç”±å™äº‹\n",
    "5. **conspiracy**: é˜´è°‹è®ºå™äº‹\n",
    "6. **memorial**: çºªå¿µä¸é—äº§å™äº‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "narrative_keywords",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– åŠ è½½è¯­ä¹‰æ¨¡å‹ç”¨äºå™äº‹æ£€æµ‹...\n",
      "ğŸ”¢ ç”Ÿæˆå™äº‹æ¡†æ¶è¯­ä¹‰å‘é‡...\n",
      "ğŸ” å¼€å§‹åŸºäºè¯­ä¹‰çš„å™äº‹æ¡†æ¶æ£€æµ‹...\n",
      "  (ä½¿ç”¨sentence embeddings + å…³é”®è¯å¢å¼º)\n",
      "\n",
      "ğŸ”¢ ç”Ÿæˆæ¨æ–‡è¯­ä¹‰å‘é‡ (2,000 æ¡)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:07<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ æ£€æµ‹å™äº‹æ¡†æ¶...\n",
      "  å¤„ç†è¿›åº¦: 2,000 / 2,000\n",
      "\n",
      "âœ… å™äº‹æ¡†æ¶æ£€æµ‹å®Œæˆ\n",
      "\n",
      "ğŸ“Š å™äº‹åˆ†å¸ƒ:\n",
      "shape: (7, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ primary_narrative  â”† count â”‚\n",
      "â”‚ ---                â”† ---   â”‚\n",
      "â”‚ str                â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ political_violence â”† 950   â”‚\n",
      "â”‚ memorial           â”† 660   â”‚\n",
      "â”‚ none               â”† 236   â”‚\n",
      "â”‚ free_speech        â”† 68    â”‚\n",
      "â”‚ consequences       â”† 42    â”‚\n",
      "â”‚ conspiracy         â”† 35    â”‚\n",
      "â”‚ polarization       â”† 9     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: 0.464\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ¤– åŠ è½½è¯­ä¹‰æ¨¡å‹ç”¨äºå™äº‹æ£€æµ‹...\")\n",
    "# å¤ç”¨ä¹‹å‰çš„æ¨¡å‹æˆ–åŠ è½½è½»é‡çº§æ¨¡å‹\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# å®šä¹‰6å¤§å™äº‹æ¡†æ¶çš„åŸå‹æ–‡æœ¬ï¼ˆæ ¸å¿ƒè¯­ä¹‰æè¿°ï¼‰\n",
    "narrative_prototypes = {\n",
    "    'political_violence': [\n",
    "        \"This is a tragic political assassination and act of violence\",\n",
    "        \"Charlie Kirk was a victim of political violence and murder\",\n",
    "        \"The shooting was a terrible attack on a political figure\",\n",
    "        \"This assassination is an act of terror against conservatives\"\n",
    "    ],\n",
    "    'consequences': [\n",
    "        \"His hateful rhetoric had dangerous consequences\",\n",
    "        \"This is the result of divisive and toxic speech\",\n",
    "        \"He deserves blame for spreading hate and division\",\n",
    "        \"His inflammatory words caused this violence\"\n",
    "    ],\n",
    "    'polarization': [\n",
    "        \"America is deeply divided and polarized\",\n",
    "        \"This shows our country is on the brink of civil war\",\n",
    "        \"We treat each other as enemies instead of fellow citizens\",\n",
    "        \"Political tribalism is tearing our nation apart\"\n",
    "    ],\n",
    "    'free_speech': [\n",
    "        \"This is an attack on free speech and open debate\",\n",
    "        \"They are trying to silence conservative voices\",\n",
    "        \"We must defend the right to express political views\",\n",
    "        \"Censorship and suppression of speech led to this\"\n",
    "    ],\n",
    "    'conspiracy': [\n",
    "        \"This was a false flag operation and setup\",\n",
    "        \"The deep state planned this assassination\",\n",
    "        \"This is a psyop to manipulate public opinion\",\n",
    "        \"The official story is fake and a coverup\"\n",
    "    ],\n",
    "    'memorial': [\n",
    "        \"We honor and remember Charlie Kirk's legacy\",\n",
    "        \"His impact on conservative youth will not be forgotten\",\n",
    "        \"Rest in peace, he made a difference in politics\",\n",
    "        \"We pay tribute to his memory and contributions\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ç”Ÿæˆå™äº‹åŸå‹çš„embeddingsï¼ˆæ¯ä¸ªå™äº‹ç”¨å…¶åŸå‹æ–‡æœ¬çš„å¹³å‡embeddingï¼‰\n",
    "print(\"ğŸ”¢ ç”Ÿæˆå™äº‹æ¡†æ¶è¯­ä¹‰å‘é‡...\")\n",
    "narrative_embeddings = {}\n",
    "for narrative, prototype_texts in narrative_prototypes.items():\n",
    "    proto_embs = semantic_model.encode(prototype_texts)\n",
    "    # ä½¿ç”¨å¹³å‡å‘é‡ä½œä¸ºè¯¥å™äº‹çš„ä»£è¡¨\n",
    "    narrative_embeddings[narrative] = np.mean(proto_embs, axis=0)\n",
    "\n",
    "# å…³é”®è¯è¾…åŠ©ï¼ˆç”¨äºå¢å¼ºconfidenceï¼‰\n",
    "narrative_keywords = {\n",
    "    'political_violence': [\n",
    "        r'\\bvictim\\b', r'\\btragedy\\b', r'\\bassassinat\\w*\\b', r'\\bviolence\\b', \n",
    "        r'\\bmurder\\w*\\b', r'\\bkill\\w*\\b', r'\\bshot\\b', r'\\bshooting\\b',\n",
    "        r'\\bterror\\w*\\b', r'\\bgunman\\b', r'\\battack\\w*\\b'\n",
    "    ],\n",
    "    'consequences': [\n",
    "        r'\\brhetoric\\b', r'\\bconsequences\\b', r'\\bhate speech\\b', r'\\bdivisive\\b',\n",
    "        r'\\bresponsib\\w*\\b', r'\\bblame\\b', r'\\bcaused\\b', r'\\bdeserve\\w*\\b',\n",
    "        r'\\bkarma\\b', r'\\breap\\w*\\b'\n",
    "    ],\n",
    "    'polarization': [\n",
    "        r'\\bdivided\\b', r'\\bpolari\\w*\\b', r'\\bcivil war\\b', r'\\benemy\\b',\n",
    "        r'\\bus vs them\\b', r'\\btear\\w* apart\\b', r'\\bpartisan\\b'\n",
    "    ],\n",
    "    'free_speech': [\n",
    "        r'\\bfree speech\\b', r'\\bsilenc\\w*\\b', r'\\bcensor\\w*\\b', r'\\bdebate\\b',\n",
    "        r'\\bfirst amendment\\b', r'\\bvoice\\b', r'\\bspeak\\w* out\\b'\n",
    "    ],\n",
    "    'conspiracy': [\n",
    "        r'\\bfalse flag\\b', r'\\bsetup\\b', r'\\bdeep state\\b', r'\\bpsyop\\b',\n",
    "        r'\\bcoverup\\b', r'\\bcover-up\\b', r'\\bplanned\\b', r'\\binside job\\b',\n",
    "        r'\\bfake\\b', r'\\bhoax\\b'\n",
    "    ],\n",
    "    'memorial': [\n",
    "        r'\\blegacy\\b', r'\\bremember\\b', r'\\bhonor\\b', r'\\bimpact\\b',\n",
    "        r'\\bRIP\\b', r'\\brest in peace\\b', r'\\bmemory\\b', r'\\bmemorial\\b',\n",
    "        r'\\btribute\\b', r'\\bmiss\\w*\\b'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def detect_narratives_semantic(text, text_embedding):\n",
    "    \"\"\"åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ + å…³é”®è¯å¢å¼ºçš„å™äº‹æ£€æµ‹\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    narrative_scores = {}\n",
    "    \n",
    "    for narrative in narrative_prototypes.keys():\n",
    "        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆä¸»è¦ï¼‰\n",
    "        similarity = cosine_similarity(\n",
    "            text_embedding.reshape(1, -1),\n",
    "            narrative_embeddings[narrative].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆè¾…åŠ©å¢å¼ºï¼‰\n",
    "        keyword_matches = sum(1 for pattern in narrative_keywords[narrative] \n",
    "                             if re.search(pattern, text_lower))\n",
    "        keyword_boost = keyword_matches * 0.05  # æ¯ä¸ªå…³é”®è¯å¢åŠ 5%\n",
    "        \n",
    "        # ç»¼åˆåˆ†æ•°ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ä¸ºä¸»ï¼Œå…³é”®è¯æä¾›boost\n",
    "        final_score = similarity + keyword_boost\n",
    "        narrative_scores[narrative] = final_score\n",
    "    \n",
    "    return narrative_scores\n",
    "\n",
    "print(\"ğŸ” å¼€å§‹åŸºäºè¯­ä¹‰çš„å™äº‹æ¡†æ¶æ£€æµ‹...\")\n",
    "print(\"  (ä½¿ç”¨sentence embeddings + å…³é”®è¯å¢å¼º)\")\n",
    "\n",
    "# ç”Ÿæˆæ‰€æœ‰æ¨æ–‡çš„embeddingsï¼ˆæ‰¹é‡å¤„ç†ï¼‰\n",
    "print(f\"\\nğŸ”¢ ç”Ÿæˆæ¨æ–‡è¯­ä¹‰å‘é‡ ({len(texts):,} æ¡)...\")\n",
    "tweet_embeddings = semantic_model.encode(texts, show_progress_bar=True, batch_size=128)\n",
    "\n",
    "# å¯¹æ¯æ¡æ¨æ–‡è¿›è¡Œå™äº‹æ£€æµ‹\n",
    "print(\"\\nğŸ¯ æ£€æµ‹å™äº‹æ¡†æ¶...\")\n",
    "narrative_results = []\n",
    "for i, (text, embedding) in enumerate(zip(texts, tweet_embeddings)):\n",
    "    scores = detect_narratives_semantic(text, embedding)\n",
    "    narrative_results.append(scores)\n",
    "    \n",
    "    if (i + 1) % 2000 == 0:\n",
    "        print(f\"  å¤„ç†è¿›åº¦: {i + 1:,} / {len(texts):,}\")\n",
    "\n",
    "# æå–ä¸»å¯¼å™äº‹ï¼ˆå¾—åˆ†æœ€é«˜çš„ï¼Œä¸”é«˜äºé˜ˆå€¼0.3ï¼‰\n",
    "primary_narratives = []\n",
    "narrative_confidences = []\n",
    "for scores in narrative_results:\n",
    "    max_narrative = max(scores, key=scores.get)\n",
    "    max_score = scores[max_narrative]\n",
    "    \n",
    "    if max_score > 0.3:  # ç½®ä¿¡åº¦é˜ˆå€¼\n",
    "        primary_narratives.append(max_narrative)\n",
    "        narrative_confidences.append(max_score)\n",
    "    else:\n",
    "        primary_narratives.append('none')  # æ— æ˜æ˜¾å™äº‹\n",
    "        narrative_confidences.append(0.0)\n",
    "\n",
    "# æ·»åŠ å™äº‹åˆ†æ•°åˆ—\n",
    "narrative_cols = {}\n",
    "for narrative in narrative_prototypes.keys():\n",
    "    narrative_cols[f'narrative_{narrative}'] = [r[narrative] for r in narrative_results]\n",
    "\n",
    "df_sample = df_sample.with_columns([\n",
    "    pl.Series('primary_narrative', primary_narratives),\n",
    "    pl.Series('narrative_confidence', narrative_confidences),\n",
    "    *[pl.Series(k, v) for k, v in narrative_cols.items()]\n",
    "])\n",
    "\n",
    "print(f\"\\nâœ… å™äº‹æ¡†æ¶æ£€æµ‹å®Œæˆ\")\n",
    "print(f\"\\nğŸ“Š å™äº‹åˆ†å¸ƒ:\")\n",
    "print(df_sample.group_by('primary_narrative').agg(pl.len().alias('count')).sort('count', descending=True))\n",
    "\n",
    "print(f\"\\nğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: {np.mean([c for c in narrative_confidences if c > 0]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stance_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 4: æ”¿æ²»ç«‹åœºåˆ†ç±»\n",
    "\n",
    "åŸºäºå…³é”®è¯çš„ç«‹åœºæ£€æµ‹ï¼š\n",
    "- **conservative**: ä¿å®ˆæ´¾ï¼ˆæ”¯æŒKirkã€è°´è´£æš´åŠ›ã€æå«ä¿å®ˆä»·å€¼ï¼‰\n",
    "- **liberal**: è‡ªç”±æ´¾ï¼ˆæ‰¹è¯„Kirkã€åæ€è¨€è®ºã€å¼ºè°ƒåæœï¼‰\n",
    "- **neutral**: ä¸­ç«‹ï¼ˆå®¢è§‚æŠ¥é“ã€å­¦æœ¯åˆ†æã€æ‚¼å¿µï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stance_detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å¼€å§‹æ”¿æ²»ç«‹åœºåˆ†ç±»...\n",
      "âœ… æ”¿æ²»ç«‹åœºåˆ†ç±»å®Œæˆ\n",
      "\n",
      "ğŸ“Š ç«‹åœºåˆ†å¸ƒ:\n",
      "shape: (3, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ political_stance â”† count â”‚\n",
      "â”‚ ---              â”† ---   â”‚\n",
      "â”‚ str              â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ neutral          â”† 1843  â”‚\n",
      "â”‚ liberal          â”† 88    â”‚\n",
      "â”‚ conservative     â”† 69    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# æ”¿æ²»ç«‹åœºå…³é”®è¯\n",
    "stance_keywords = {\n",
    "    'conservative': [\n",
    "        r'\\bhero\\b', r'\\bpatriot\\b', r'\\bfreedom fighter\\b', r'\\bdefend\\w*\\b',\n",
    "        r'\\bMAGA\\b', r'\\bTrump\\b', r'\\bconservative movement\\b',\n",
    "        r'\\bleft\\w* violence\\b', r'\\bsocialist\\w*\\b', r'\\bliberal\\w* violence\\b',\n",
    "        r'\\bmarty\\w*\\b', r'\\bstanding up\\b'\n",
    "    ],\n",
    "    'liberal': [\n",
    "        r'\\bhateful\\b', r'\\btoxic\\b', r'\\bdangerous rhetoric\\b',\n",
    "        r'\\bextremis\\w*\\b', r'\\bhate speech\\b', r'\\bconsequences\\b',\n",
    "        r'\\bdeserve\\w*\\b', r'\\breap what\\b', r'\\bfar-right\\b',\n",
    "        r'\\bTurning Point\\b.*\\bnegative\\b'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def detect_stance(text):\n",
    "    \"\"\"æ£€æµ‹æ”¿æ²»ç«‹åœº\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    conservative_score = sum(1 for pattern in stance_keywords['conservative'] if re.search(pattern, text_lower))\n",
    "    liberal_score = sum(1 for pattern in stance_keywords['liberal'] if re.search(pattern, text_lower))\n",
    "    \n",
    "    if conservative_score > liberal_score and conservative_score > 0:\n",
    "        return 'conservative', conservative_score\n",
    "    elif liberal_score > conservative_score and liberal_score > 0:\n",
    "        return 'liberal', liberal_score\n",
    "    else:\n",
    "        return 'neutral', 0\n",
    "\n",
    "print(\"ğŸ¯ å¼€å§‹æ”¿æ²»ç«‹åœºåˆ†ç±»...\")\n",
    "stance_results = [detect_stance(t) for t in texts]\n",
    "stances = [r[0] for r in stance_results]\n",
    "stance_scores = [r[1] for r in stance_results]\n",
    "\n",
    "df_sample = df_sample.with_columns([\n",
    "    pl.Series('political_stance', stances),\n",
    "    pl.Series('stance_confidence', stance_scores)\n",
    "])\n",
    "\n",
    "print(f\"âœ… æ”¿æ²»ç«‹åœºåˆ†ç±»å®Œæˆ\")\n",
    "print(f\"\\nğŸ“Š ç«‹åœºåˆ†å¸ƒ:\")\n",
    "print(df_sample.group_by('political_stance').agg(pl.len().alias('count')).sort('count', descending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 5: æ—¶é—´æ¼”å˜åˆ†æ\n",
    "\n",
    "åˆ†ææƒ…æ„Ÿä¸å™äº‹åœ¨5ä¸ªæ—¶é—´çª—å£çš„æ¼”å˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "temporal_evolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ æ—¶é—´æ¼”å˜åˆ†æ\n",
      "\n",
      "ğŸ­ æƒ…æ„Ÿæ¼”å˜ (å¹³å‡åˆ†æ•°):\n",
      "shape: (1, 8)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ time_window â”† tweet_count â”† avg_sadnes â”† avg_anger â”† avg_fear â”† avg_surpri â”† avg_joy  â”† avg_love â”‚\n",
      "â”‚ ---         â”† ---         â”† s          â”† ---       â”† ---      â”† se         â”† ---      â”† ---      â”‚\n",
      "â”‚ str         â”† u32         â”† ---        â”† f64       â”† f64      â”† ---        â”† f64      â”† f64      â”‚\n",
      "â”‚             â”†             â”† f64        â”†           â”†          â”† f64        â”†          â”†          â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 48-72h      â”† 2000        â”† 0.144821   â”† 0.217268  â”† 0.192494 â”† 0.126924   â”† 0.074406 â”† 0.0      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“– å™äº‹æ¼”å˜ (å„æ—¶æ®µtop3å™äº‹):\n",
      "\n",
      "  0-6h:\n",
      "\n",
      "  6-12h:\n",
      "\n",
      "  12-24h:\n",
      "\n",
      "  24-48h:\n",
      "\n",
      "  48-72h:\n",
      "    - political_violence: 950 æ¡\n",
      "    - memorial: 660 æ¡\n",
      "    - none: 236 æ¡\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“ˆ æ—¶é—´æ¼”å˜åˆ†æ\")\n",
    "\n",
    "# æƒ…æ„Ÿéšæ—¶é—´æ¼”å˜\n",
    "emotion_evolution = df_sample.group_by('time_window').agg([\n",
    "    pl.len().alias('tweet_count'),\n",
    "    pl.col('emotion_sadness').mean().alias('avg_sadness'),\n",
    "    pl.col('emotion_anger').mean().alias('avg_anger'),\n",
    "    pl.col('emotion_fear').mean().alias('avg_fear'),\n",
    "    pl.col('emotion_surprise').mean().alias('avg_surprise'),\n",
    "    pl.col('emotion_joy').mean().alias('avg_joy'),\n",
    "    pl.col('emotion_love').mean().alias('avg_love')\n",
    "]).sort('time_window')\n",
    "\n",
    "print(\"\\nğŸ­ æƒ…æ„Ÿæ¼”å˜ (å¹³å‡åˆ†æ•°):\")\n",
    "print(emotion_evolution)\n",
    "\n",
    "# å™äº‹éšæ—¶é—´æ¼”å˜\n",
    "narrative_evolution = df_sample.group_by(['time_window', 'primary_narrative']).agg(\n",
    "    pl.len().alias('count')\n",
    ").sort(['time_window', 'count'], descending=[False, True])\n",
    "\n",
    "print(\"\\nğŸ“– å™äº‹æ¼”å˜ (å„æ—¶æ®µtop3å™äº‹):\")\n",
    "for window in ['0-6h', '6-12h', '12-24h', '24-48h', '48-72h']:\n",
    "    top_narratives = narrative_evolution.filter(pl.col('time_window') == window).head(3)\n",
    "    print(f\"\\n  {window}:\")\n",
    "    for row in top_narratives.iter_rows(named=True):\n",
    "        print(f\"    - {row['primary_narrative']}: {row['count']} æ¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 6: æå–ä»£è¡¨æ€§æ¨æ–‡\n",
    "\n",
    "æ¯ç±»å™äº‹é€‰æ‹©2æ¡æœ€å…·ä»£è¡¨æ€§çš„æ¨æ–‡ï¼ˆåŸºäºengagementå’Œå™äº‹å¾—åˆ†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "representative_tweets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æå–ä»£è¡¨æ€§æ¨æ–‡...\n",
      "\n",
      "âœ… ä»£è¡¨æ€§æ¨æ–‡æå–å®Œæˆ\n",
      "\n",
      "ğŸ† å„å™äº‹ä»£è¡¨æ€§æ¨æ–‡ï¼ˆå‰100å­—ç¬¦ï¼‰:\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“ æå–ä»£è¡¨æ€§æ¨æ–‡...\")\n",
    "\n",
    "# è®¡ç®—engagementåˆ†æ•°\n",
    "df_sample = df_sample.with_columns(\n",
    "    (pl.col('retweetCount') + pl.col('likeCount') * 0.5 + pl.col('replyCount') * 0.3).alias('engagement_score')\n",
    ")\n",
    "\n",
    "representative_tweets = {}\n",
    "\n",
    "for narrative in narrative_keywords.keys():\n",
    "    # ç­›é€‰è¯¥å™äº‹çš„æ¨æ–‡\n",
    "    narrative_tweets = df_sample.filter(\n",
    "        (pl.col('primary_narrative') == narrative) &\n",
    "        (pl.col(f'narrative_{narrative}') >= 2)  # è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯\n",
    "    ).sort('engagement_score', descending=True).head(2)\n",
    "    \n",
    "    if narrative_tweets.height > 0:\n",
    "        representative_tweets[narrative] = narrative_tweets.select(['text', 'engagement_score', 'primary_emotion']).to_dicts()\n",
    "\n",
    "print(f\"\\nâœ… ä»£è¡¨æ€§æ¨æ–‡æå–å®Œæˆ\")\n",
    "print(f\"\\nğŸ† å„å™äº‹ä»£è¡¨æ€§æ¨æ–‡ï¼ˆå‰100å­—ç¬¦ï¼‰:\")\n",
    "for narrative, tweets in representative_tweets.items():\n",
    "    print(f\"\\nã€{narrative.upper()}ã€‘\")\n",
    "    for i, tweet in enumerate(tweets, 1):\n",
    "        print(f\"  {i}. [{tweet['primary_emotion']}] {tweet['text'][:100]}...\")\n",
    "        print(f\"     Engagement: {tweet['engagement_score']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 7: ä¿å­˜åˆ†æç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "save_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å†…å®¹åˆ†æç»“æœå·²ä¿å­˜: ../parquet/content_analysis.parquet\n",
      "âœ… æƒ…æ„Ÿæ¼”å˜æ•°æ®å·²ä¿å­˜: ../parquet/emotion_evolution.parquet\n",
      "âœ… å™äº‹æ¼”å˜æ•°æ®å·²ä¿å­˜: ../parquet/narrative_evolution.parquet\n",
      "\n",
      "ğŸ“Š æ•°æ®æ¦‚è§ˆ:\n",
      "  æ€»åˆ†ææ¨æ–‡æ•°: 2,000\n",
      "  æ—¶é—´çª—å£æ•°: 5\n",
      "  æƒ…æ„Ÿç»´åº¦: 6\n",
      "  å™äº‹æ¡†æ¶: 6\n",
      "  æ”¿æ²»ç«‹åœº: 3\n"
     ]
    }
   ],
   "source": [
    "from src import io\n",
    "\n",
    "# ä¿å­˜å®Œæ•´çš„å†…å®¹åˆ†ææ•°æ®\n",
    "content_path = Path(\"../parquet/content_analysis.parquet\")\n",
    "io.materialize_parquet(df_sample.lazy(), content_path)\n",
    "print(f\"âœ… å†…å®¹åˆ†æç»“æœå·²ä¿å­˜: {content_path}\")\n",
    "\n",
    "# ä¿å­˜æƒ…æ„Ÿæ¼”å˜æ•°æ®\n",
    "emotion_evo_path = Path(\"../parquet/emotion_evolution.parquet\")\n",
    "io.materialize_parquet(emotion_evolution.lazy(), emotion_evo_path)\n",
    "print(f\"âœ… æƒ…æ„Ÿæ¼”å˜æ•°æ®å·²ä¿å­˜: {emotion_evo_path}\")\n",
    "\n",
    "# ä¿å­˜å™äº‹æ¼”å˜æ•°æ®\n",
    "narrative_evo_path = Path(\"../parquet/narrative_evolution.parquet\")\n",
    "io.materialize_parquet(narrative_evolution.lazy(), narrative_evo_path)\n",
    "print(f\"âœ… å™äº‹æ¼”å˜æ•°æ®å·²ä¿å­˜: {narrative_evo_path}\")\n",
    "\n",
    "# ä¿å­˜ä»£è¡¨æ€§æ¨æ–‡ï¼ˆè½¬ä¸ºDataFrameï¼‰\n",
    "repr_tweets_list = []\n",
    "for narrative, tweets in representative_tweets.items():\n",
    "    for tweet in tweets:\n",
    "        repr_tweets_list.append({\n",
    "            'narrative': narrative,\n",
    "            'text': tweet['text'],\n",
    "            'emotion': tweet['primary_emotion'],\n",
    "            'engagement': tweet['engagement_score']\n",
    "        })\n",
    "\n",
    "if repr_tweets_list:\n",
    "    repr_tweets_df = pl.DataFrame(repr_tweets_list)\n",
    "    repr_tweets_path = Path(\"../parquet/representative_tweets.parquet\")\n",
    "    io.materialize_parquet(repr_tweets_df.lazy(), repr_tweets_path)\n",
    "    print(f\"âœ… ä»£è¡¨æ€§æ¨æ–‡å·²ä¿å­˜: {repr_tweets_path}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "print(f\"  æ€»åˆ†ææ¨æ–‡æ•°: {df_sample.height:,}\")\n",
    "print(f\"  æ—¶é—´çª—å£æ•°: 5\")\n",
    "print(f\"  æƒ…æ„Ÿç»´åº¦: 6\")\n",
    "print(f\"  å™äº‹æ¡†æ¶: 6\")\n",
    "print(f\"  æ”¿æ²»ç«‹åœº: 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## âœ… å†…å®¹åˆ†æå®Œæˆï¼\n",
    "\n",
    "**ç”Ÿæˆçš„æ ¸å¿ƒæ•°æ®**:\n",
    "- `content_analysis.parquet`: å®Œæ•´çš„æƒ…æ„Ÿã€å™äº‹ã€ç«‹åœºåˆ†æç»“æœ\n",
    "- `emotion_evolution.parquet`: 6å¤§æƒ…æ„Ÿéšæ—¶é—´çš„æ¼”å˜\n",
    "- `narrative_evolution.parquet`: 6å¤§å™äº‹éšæ—¶é—´çš„æ¼”å˜\n",
    "- `representative_tweets.parquet`: å„å™äº‹çš„ä»£è¡¨æ€§æ¨æ–‡\n",
    "\n",
    "**ä¸‹ä¸€æ­¥**: \n",
    "1. è¿è¡Œ `01_temporal_dynamics.ipynb` ç”Ÿæˆå°æ—¶çº§æ—¶é—´åºåˆ—\n",
    "2. æ„å»ºå¯è§†åŒ–Dashboardå±•ç¤ºæ‰€æœ‰æ´å¯Ÿ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
